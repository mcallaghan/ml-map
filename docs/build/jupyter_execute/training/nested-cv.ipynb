{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b1dae7",
   "metadata": {},
   "source": [
    "# Nested cross-validation\n",
    "\n",
    "In the previous example, we found out how we can test how a specific set of hyparameters works on a test dataset. If we want to know how each set of possible hyperparameters works, we could test each of these individually.\n",
    "\n",
    ":::{danger}\n",
    "Once we have used a set of documents to optimize anything, it is no longer a valid test dataset. We can no longer use this data to estimate our model's performance on new data.\n",
    ":::\n",
    "\n",
    "The reason is that we are in danger of **overfitting**, or over-optimizing for our specific test dataset, at the expense of generalisability on new data.\n",
    "\n",
    "## Train, Validation, Test data\n",
    "\n",
    "To avoid this, we need to split the data further. We end up with\n",
    "\n",
    "- *Training Data*: This is used to train models and model specifications\n",
    "- *Validation Data*: This is used to find out which model specification we think works best\n",
    "- *Testing Data*: This is used to test the model we think works best and estimate its performance on new data\n",
    "\n",
    "Each data point should only ever appear in one of these.\n",
    "\n",
    "Let's see a simplified example. First we need to split our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d5817",
   "metadata": {
    "tags": [
     "hide-cell",
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../../')\n",
    "\n",
    "params = {\n",
    "  \"per_device_train_batch_size\": [16, 32],\n",
    "  \"learning_rate\": [5e-5, 3e-5, 2e-5],\n",
    "  \"num_train_epochs\": [2,3,4]\n",
    "}\n",
    "\n",
    "import itertools\n",
    "def product_dict(**kwargs):\n",
    "    keys = kwargs.keys()\n",
    "    vals = kwargs.values()\n",
    "    for instance in itertools.product(*vals):\n",
    "        yield dict(zip(keys, instance))\n",
    "param_list = list(product_dict(**params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90730083",
   "metadata": {
    "tags": [
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Let's imagine we have a dataset with 100 documents\n",
    "idx = np.arange(100)\n",
    "\n",
    "# split our whole data into train and test sets\n",
    "train_idx, test_idx = train_test_split(idx, shuffle=False)\n",
    "# set aside some validation by removing from training data\n",
    "sub_train_idx, val_idx = train_test_split(train_idx, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21107a94",
   "metadata": {},
   "source": [
    "We can visualise these splits with coloured dots, where each dot represents a paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f798b1",
   "metadata": {
    "tags": [
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(dpi=125, figsize=(8,1))\n",
    "\n",
    "s = 3\n",
    "ax.scatter(\n",
    "    sub_train_idx, [0]*sub_train_idx.size,\n",
    "    color=\"C0\", s=s, label=\"Train\",\n",
    "    marker=\"s\"\n",
    ")\n",
    "ax.scatter(\n",
    "    val_idx, [0]*val_idx.size,\n",
    "    color=\"C1\", s=s, label=\"Validation\",\n",
    "    marker=\"s\"\n",
    ")\n",
    "ax.scatter(\n",
    "    test_idx, [0]*test_idx.size,\n",
    "    color=\"C2\", s=s, label=\"Test\",\n",
    "    marker=\"s\"\n",
    ")\n",
    "ax.set_yticks([])\n",
    "ax.legend(bbox_to_anchor=(0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d37bd47",
   "metadata": {},
   "source": [
    "Simplified code to find the parameters that work best on our validation data would look like this\n",
    "\n",
    ":::{attention}\n",
    "For illustrative purposes, in each the following examples we will simply make up\n",
    "random values when \"evaluating\" models. The train and eval functions are simply stand-ins to show the process\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54987e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our train function would train a model withing parameters p on data train_idx\n",
    "def train(p, train_idx):\n",
    "    return None\n",
    "\n",
    "# Our eval function would calculate the f1 score for model predictions for our validation/test data\n",
    "def eval(model, val_idx):\n",
    "    return random.random() # here we simply return a random number\n",
    "\n",
    "scores = []\n",
    "# loop through our different parameter settings\n",
    "for p in param_list:\n",
    "    # Train a model with these parameters on the training data\n",
    "    model = train(p, sub_train_idx)\n",
    "    # Evaluate this model on the validation data\n",
    "    scores.append(eval(model, val_idx))\n",
    "\n",
    "# Now we can find the set of parameters which had the best score\n",
    "top_idx = np.argmax(scores)\n",
    "best_params = param_list[top_idx]\n",
    "print(best_params)\n",
    "print(f'F1 score {scores[top_idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6410a2f",
   "metadata": {},
   "source": [
    "Once we have found our best set of parameters, we can train our final model on both train and validation data, and evaluate it on our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc577398",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(best_params, train_idx)\n",
    "f1_estimate = eval(model, test_idx)\n",
    "print(f1_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd9f8ed",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Each time we sample data, a random element of noise creeps into our process.\n",
    "To reduce the influence of random noise, we can employ cross-validation in order make our process more robust.\n",
    "\n",
    "One way of doing this, is to split training dataset into multiple train-validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf864df5",
   "metadata": {
    "tags": [
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(dpi=125, figsize=(8,4))\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "k = 5\n",
    "kf = KFold(n_splits=k)\n",
    "\n",
    "for i, (sub_train_idx, val_idx) in enumerate(kf.split(train_idx)):\n",
    "\n",
    "    ax.scatter(\n",
    "        train_idx[sub_train_idx], [i]*sub_train_idx.size,\n",
    "        color=\"C0\", s=s, label=\"Train\" if i == 0 else \"\",\n",
    "        marker=\"s\"\n",
    "    )\n",
    "    ax.scatter(\n",
    "        train_idx[val_idx], [i]*val_idx.size,\n",
    "        color=\"C1\", s=s, label=\"Validation\" if i == 0 else \"\",\n",
    "        marker=\"s\"\n",
    "    )\n",
    "\n",
    "ax.scatter(\n",
    "    test_idx, [0]*test_idx.size,\n",
    "    color=\"C2\", s=s, label=\"Test\",\n",
    "    marker=\"s\"\n",
    ")\n",
    "ax.set_yticks([])\n",
    "ax.legend(bbox_to_anchor=(0,1))\n",
    "plt.show()\n",
    "\n",
    "ax.set_yticks([])\n",
    "ax.legend(bbox_to_anchor=(0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ad43fc",
   "metadata": {},
   "source": [
    "Each split of training data into training and validation sets is called a **fold**, hence [k-fold cross-validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html).\n",
    "\n",
    "To find our best model using k-fold cross-validation, we will evaluate each set of parameters on each fold. Then we will select the set of parameters that achieved the highest f1-score *on average* across the folds.\n",
    "\n",
    "Simplified code that explains this process looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef86d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores = np.empty((len(param_list), k))\n",
    "\n",
    "kf = KFold(n_splits=k)\n",
    "\n",
    "\n",
    "# loop through our different parameter settings\n",
    "for i, p in enumerate(param_list):\n",
    "    # loop through each split\n",
    "    for j, (sub_train_idx, val_idx) in enumerate(kf.split(train_idx)):\n",
    "        # Train a model with these parameters on the training data\n",
    "        model = train(p, train_idx[sub_train_idx])\n",
    "        # Evaluate this model on the validation data\n",
    "        scores[i,j] = eval(model, train_idx[val_idx])\n",
    "\n",
    "# Now we can find the set of parameters which had the best score averaged across splits\n",
    "mean_scores = scores.mean(axis=1)\n",
    "top_idx = np.argmax(mean_scores)\n",
    "best_params = param_list[top_idx]\n",
    "print(best_params)\n",
    "print(f'F1 score {mean_scores[top_idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa185e90",
   "metadata": {},
   "source": [
    "Once again, we can evaluate this best model against our test data.\n",
    "\n",
    "You will notice though, that we need to train and evaluate `k` times as many models\n",
    "\n",
    "### Nested cross-validation\n",
    "\n",
    "However, our test dataset is still only one sample from  many different possible samples. If we had picked a different sample, our evaluation score may have been different.\n",
    "\n",
    "**Nested cross-validation** allows us to use each labelled document we have once, making our estimate of generalisation performance less susceptible to random noise.\n",
    "\n",
    "This is particularly useful when we have limited amounts of labelled data (with a sufficiently large test dataset this is not really necessary). However, it increases computational requirements once more.\n",
    "\n",
    "With nested CV, we have two folds: an outer fold, and an inner fold. These are split up as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c87211",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=125, figsize=(8,4))\n",
    "outer_fold = KFold(k)\n",
    "for i, (outer_train_idx, test_idx) in enumerate(outer_fold.split(idx)):\n",
    "    ax.scatter(\n",
    "        outer_train_idx, [i]*outer_train_idx.size,\n",
    "        color=\"C0\", s=2, label=\"Outer Train\" if i == 0 else \"\",\n",
    "        marker=\"s\"\n",
    "    )\n",
    "    ax.scatter(\n",
    "        test_idx, [i]*test_idx.size,\n",
    "        color=\"C1\", s=2, label=\"Outer Test\" if i == 0 else \"\",\n",
    "        marker=\"s\"\n",
    "    )\n",
    "    inner_fold = KFold()\n",
    "    for j, (inner_train_idx, inner_validation_idx) in enumerate(inner_fold.split(outer_train_idx)):\n",
    "        ax.scatter(\n",
    "            outer_train_idx[inner_train_idx], [i+(j+1)*0.1]*inner_train_idx.size,\n",
    "            color=\"C2\", s=2, label=\"Inner Train\" if i+j == 0 else \"\",\n",
    "            marker=\"s\"\n",
    "        )\n",
    "        ax.scatter(\n",
    "            outer_train_idx[inner_validation_idx], [i+(j+1)*0.1]*inner_validation_idx.size,\n",
    "            color=\"C3\", s=2, label=\"Inner validation\" if i+j == 0 else \"\",\n",
    "            marker=\"s\"\n",
    "        )\n",
    "\n",
    "ax.set_yticks(np.arange(i+1))\n",
    "ax.legend(bbox_to_anchor=(0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f381987",
   "metadata": {},
   "source": [
    "We simply repeat the cross-validation process we saw before `k` times, each time assessing the model we find best on a different outer test set.\n",
    "\n",
    "We will end up with `k` separate estimates of our f1 score. We can report the mean of these, with the standard deviation indicating how much this estimate varies across different test sets.\n",
    "\n",
    "#### Final model\n",
    "\n",
    "However, if we follow the process above, we end up with 5 best models, each of which may have a different set of hyperparameters.\n",
    "\n",
    "In order to calculate our final model, we need to do one more pass through the outer folds, training a model for each set of hyperparameters on each outer train dataset, and evaluating on each outer test dataset. The model that achieves the best score across those test datasets is the one we will use.\n",
    "\n",
    ":::{attention}\n",
    "Our final model may end up different from some or all of the models we used to estimate our f1 score. What we are doing is not evaluating the model itself but the *model selection procedure*\n",
    ":::\n",
    "\n",
    "In the next section we will find out how to make this process more efficient"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.12,
    "jupytext_version": "1.8.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   13,
   37,
   60,
   75,
   79,
   103,
   112,
   135,
   139,
   144,
   153,
   187,
   195,
   218,
   234,
   265
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}