{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4860ccd2",
   "metadata": {},
   "source": [
    "# Putting our pipeline together\n",
    "\n",
    "We now have all the elements we need to come up with a final model, estimate its performance, and use it to make predictions about the data we have not labelled.\n",
    "\n",
    "Running this pipeline on the full dataset is computationally intensive, so it is written as set of python scripts.\n",
    "\n",
    "`run_full_pipeline.sh` will run the whole training, evaluation and prediction pipeline for each target variable(s). You can change the model name variable in the script to run the pipeline with different models\n",
    "\n",
    "{meth}`mlmap.pipeline_train` trains and evaluates a given model on a given target variable. It saves the final model, as well as evaluation scores and predictions made for the outer test sets in the `results` directory.\n",
    "\n",
    "Run `python mlmap/pipeline_train.py -h` to see the possible arguments\n",
    "\n",
    "{meth}`mlmap.pipeline_predict` takes the saved model, and makes predictions for documents that do not have labels\n",
    "\n",
    "## Trial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f9d13",
   "metadata": {
    "tags": [
     "remove-cell",
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc11f25",
   "metadata": {},
   "source": [
    "The results of our trials are stored in `results/trials.db`. We can inspect these as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dbecd0",
   "metadata": {
    "tags": [
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "from mlmap import return_search\n",
    "db = \"results/trials.db\"\n",
    "model_name = \"distilroberta-base\"\n",
    "y_prefix = \"INCLUDE\"\n",
    "study_name = f\"{model_name}__{y_prefix}\"\n",
    "df = return_search(db, study_name)\n",
    "# Number of trials completed\n",
    "print(df.number.unique().shape[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ad9d32",
   "metadata": {},
   "source": [
    "We can see how long it took to get to the highest value in our set of trials in each outer fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf590f94",
   "metadata": {
    "tags": [
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "for fold, group in df.groupby(\"study_name\"):\n",
    "    f1_max = group.groupby('number')['value'].max().cummax()\n",
    "    ax.plot(f1_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c00b8",
   "metadata": {},
   "source": [
    "## Final model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13a872e",
   "metadata": {
    "tags": [
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "model = 'distilroberta-base'\n",
    "\n",
    "ys = [\"INCLUDE\",\"4 -\",\"8 -\"]\n",
    "scores = []\n",
    "for y in ys:\n",
    "    for k in range(3):\n",
    "        study_name = f\"{model_name}__{y}__{k}\"\n",
    "        try:\n",
    "            with open(f\"results/{study_name}.json\", \"r\") as f:\n",
    "                res = json.load(f)\n",
    "                try:\n",
    "                    res = {**res[\"hyperparameters\"],**res[\"scores\"]}\n",
    "                except:\n",
    "                    print(\"Could not load evaluated_scores\")\n",
    "                    continue\n",
    "                res[\"model\"] = model_name\n",
    "                res[\"y\"] = y\n",
    "                res[\"k\"] = k\n",
    "                scores.append(res)\n",
    "        except FileNotFoundError:\n",
    "            print(\"Could not find file\")\n",
    "\n",
    "score_df = pd.DataFrame.from_dict(scores)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4706b32",
   "metadata": {},
   "source": [
    "We can see that the model hyperparameters are slightly different across each outer fold.\n",
    "\n",
    "Now let's tidy it up into a nice table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e6680f",
   "metadata": {
    "tags": [
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cdict = {\n",
    "    \"INCLUDE\": \"1. Relevance\",\n",
    "    \"4 -\": \"2. Instrument I\",\n",
    "    \"8 -\": \"3. Sector\",\n",
    "}\n",
    "\n",
    "def mean_std(x):\n",
    "    m = np.mean(x)\n",
    "    sd = np.std(x)\n",
    "    return f\"{m:.2} ({sd:.2})\"\n",
    "score_tab = score_df.pivot_table(index=\"y\",columns=\"model\",values=\"eval_f1\", aggfunc=mean_std).reset_index()\n",
    "\n",
    "score_tab.columns.name = None\n",
    "score_tab[\"y\"] = score_tab[\"y\"].apply(lambda x: cdict[x])\n",
    "score_tab = score_tab.rename(columns={\"y\":\"Category\"}).fillna(\"\").sort_values(\"Category\", ascending=True)\n",
    "score_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf4887b",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "Predictions are stored in `results/predictions`, we can merge these with our dataset using the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b060c9",
   "metadata": {
    "tags": [
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_categories(model_name, y_prefix, seen_df, unseen_df, min_samples=15):\n",
    "    pred = np.load(f\"results/predictions/{model_name}__{y_prefix}__predictions.npy\")\n",
    "    ids = np.load(f\"results/predictions/{model_name}__{y_prefix}__ids.npy\", allow_pickle=True)\n",
    "    cols = [x for x in seen_df.columns if re.match(y_prefix, x)]\n",
    "    cols = [x for x in cols if seen_df[x].sum()>min_samples]\n",
    "    pred_df = pd.DataFrame({\"id\": ids})\n",
    "    if y_prefix=='INCLUDE':\n",
    "        pred_df['INCLUDE']=pred\n",
    "    else:\n",
    "        for i, cname in enumerate(cols):\n",
    "            pred_df[cname]=pred[:,i]\n",
    "    df = pd.concat([\n",
    "        unseen_df.merge(pred_df, how='inner'),\n",
    "        unseen_df.merge(seen_df[['id']+cols], how='inner')\n",
    "    ]).reset_index(drop=True)\n",
    "    print(df.shape)\n",
    "    return df\n",
    "\n",
    "seen_df = pd.read_feather('data/labels.feather')\n",
    "unseen_df = pd.read_feather('data/documents.feather')\n",
    "\n",
    "\n",
    "df = get_categories(model_name, 'INCLUDE', seen_df, unseen_df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637e652",
   "metadata": {},
   "source": [
    "Documents now have a number between 0 and 1 where a prediction has been made,\n",
    "and either 0 or 1 if we have a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd3a51",
   "metadata": {
    "tags": [
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "df[df['INCLUDE'].isin([0,1])].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7162c880",
   "metadata": {},
   "source": [
    "With two more calls to the function, and after filtering only documents we think are relevant, we have our final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24737b30",
   "metadata": {
    "tags": [
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "df = df[df['INCLUDE']>=0.5]\n",
    "df = df.dropna(subset=['title','abstract'])\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df = get_categories(model_name, '4 -', seen_df, df)\n",
    "df = get_categories(model_name, '8 -', seen_df, df)\n",
    "\n",
    "df.to_feather('data/final_dataset.feather')\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.12,
    "jupytext_version": "1.8.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   13,
   31,
   36,
   40,
   52,
   57,
   67,
   71,
   101,
   109,
   130,
   136,
   166,
   171,
   176,
   180
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}