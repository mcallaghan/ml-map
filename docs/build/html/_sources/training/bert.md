# BERT and transformer-based models

BERT (Bidirectional Encoder Representations from Transformers) ([Devlin et al. 2019)](https://arxiv.org/abs/1810.04805)) has changed how we machine learning with text

Put very simply [more detail here](https://jalammar.github.io/illustrated-bert/), a large model is trained to predict missing words from billions of sentences.

![bert-diagram](../images/bert-transfer-learning.png)
